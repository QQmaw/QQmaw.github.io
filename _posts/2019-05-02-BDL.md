---
# Posts need to have the `post` layout
layout: post
comments: true

# The title of your post
title: BDL簡介 - Bidirectional Learning for Domain Adaptation of Semantic Segmentation


# (Optional) Write a short (~150 characters) description of each blog post.
# This description is used to preview the page on search engines, social media, etc.
description: >
  Yunsheng Li, Lu Yuan, Nuno Vasconcelos. ["Bidirectional Learning for Domain Adaptation of Semantic Segmentation"](https://arxiv.org/abs/1904.10620). In CVPR'19.

# (Optional) Link to an image that represents your blog post.
# The aspect ratio should be ~16:9.
<!-- image: /assets/img/default.jpg -->
<!-- hide_image: true -->

# You can hide the description and/or image from the output
# (only visible to search engines) by setting:
# hide_description: true
# hide_image: true

# (Optional) Each post can have zero or more categories, and zero or more tags.
# The difference is that categories will be part of the URL, while tags will not.
# E.g. the URL of this post is <site.baseurl>/hydejack/2017/11/23/example-content/
categories: [Deep Learning, Computer Vision]
tags: []
# If you want a category or tag to have its own page,
# check out `_featured_categories` and `_featured_tags` respectively.
---
CVPR 2019 Paper

Paper link : https://arxiv.org/abs/1904.10620

Github (May.02.2019，尚未釋出) : https://github.com/liyunsheng13/BDL


# 前備知識

如果不清楚 Domain Adaptation 或是 Domain Shift 的，

可以先看我之前寫過的 [AdaptSegNet簡介]

# 簡介

以往常見的 Domain Adaptation 的方式是透過 GAN-based 的方法，

此文也是基於此方法作延伸，

對 GAN-based 方法不清楚的可以看 [AdaptSegNet簡介]。

此文提出 Bidirectional learning framework，

其框架包含兩個部分
- Image-to-Image 模型做轉換，此處使用 Cycle GAN
- Segmentation adaptation model 其包含 Semantic Segmentation model 以及 Discriminator

Image-to-Image 的想法是將 Source domain(Ｓ) 的畫風轉換為 Target domain(Ｔ) 來減緩 domain shift 所帶來的傷害(降低準確度)。

特別的是此論文的 Image-to-Image 模型會依據 Semantic Segmentation 的結果做訓練。

因此稱為 Bidirectional learning～

![](/assets/img/2019-05-02-BDL/fig1.png)

除此之外還提出 Self-supervised(SSL) 的方式來提升準確度。



# 問題設定

通常 Unsupervised domain adaptation (UDA) 在語義分割（Semantic segmentation）的任務中的定義如下
- Source Domain：圖片 Xs，有標注正解的圖片 Ys
- Target Domain：圖片 Xt，沒標注正解圖片

# 架構

![](/assets/img/2019-05-02-BDL/fig3.png)

## Bidirectional Learning

首先訓練 Image-to-Image 的模型，

使用 S 與 T 的圖片做訓練，

訓練方式相似於以往的 Cycle-GAN。

![](/assets/img/2019-05-02-BDL/pix2pix-eq.png)

![](/assets/img/2019-05-02-BDL/eq2.png)

提出了新的 Perceptual loss !! 

使用轉換完的圖片輸入至 Semantic Segmentation 去做預測，

那直覺的想法是如果圖片轉換的夠好的話，

那麼輸入進模型的預測結果應該也會差不多，

![](/assets/img/2019-05-02-BDL/M-per-eq.png)

備註：
> F-1 可以理解使用另一個分支 T -> S，將 S'(相似 T) 轉回 S，


再來訓練 Segmentation Adaptation Model，

使用常見的 Semantic loss 以及 Adversarial 的方法訓練，

使用的事 Image-to-Image 所生成的 S' (經由 S -> T 的分支生成) 作為輸入。

![](/assets/img/2019-05-02-BDL/eq1.png)


## Self-supervised Learning(SSL)

如果 S 以及 T 的資料集都有 GT 的話，

那都使用 Fully Supervised 的方法做訓練是較佳的，

但是 DA 的問題是 T 的資料集沒有 GT，

但我們可以使用預測出來的 Yt 來做訓練，

這邊是選用 Yt 中擁有較高自信分數(> threshold)的 pixel 去做訓練，

對於有較高分數的 pixel 我們稱做 Y^t，

![](/assets/img/2019-05-02-BDL/mask-seg.png)

因此訓練 Segmentation adaptation model 會多一個 Segmentation loss，

![](/assets/img/2019-05-02-BDL/eq3.png)

![](/assets/img/2019-05-02-BDL/fig2.png)

上圖左方的 Step1 指的是使用 Segmentation adaptation model 完的效果，

而 Step2 指的是使用 Self-supervised Learning(SSL) 完的效果，


訓練流程步驟

![](/assets/img/2019-05-02-BDL/algor1.png)

![](/assets/img/2019-05-02-BDL/fig4.png)

- K 為 iteration 次數：實驗設定為 2，可看下方 table 1 實驗。
- Threshold：設定為 0.9，可看下方 table 3 實驗。
- N 為 SSL 的次數：設定為 2，可看下方 table 4 實驗。

參數細節有點多，有興趣自己去看論文理解好了。。。

![](/assets/img/2019-05-02-BDL/table1.png)

![](/assets/img/2019-05-02-BDL/table34.png)


# 成果

對於 Row 4, 5 的 mIoU 47.2 -> 44.3 那部分挺特別的，

有興趣的去看論文，講得有點抽象呢！

![](/assets/img/2019-05-02-BDL/table2.png)

![](/assets/img/2019-05-02-BDL/table56.png)



# 參考資料：

[Bidirectional Learning for Domain Adaptation of Semantic Segmentation]

[AdaptSegNet簡介]

[AdaptSegNet:Learning to Adapt Structured Output Space for Semantic Segmentation]

[Bidirectional Learning for Domain Adaptation of Semantic Segmentation]:https://arxiv.org/abs/1904.10620

[AdaptSegNet:Learning to Adapt Structured Output Space for Semantic Segmentation]:https://arxiv.org/abs/1802.10349

[AdaptSegNet簡介]:https://xiaosean.github.io/deep%20learning/computer%20vision/2018-06-20-AdaptSegNet/
